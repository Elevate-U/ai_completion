1.  Identify the AI tools: I will list the AI tools available in the `ai_tools_resource/data/` directory.
2.  Backup & Version Control: Before modifying any files, I will create a backup of the `ai_tools_resource/data/` directory and the `website/` directory.
3.  Gather existing data: I will read the JSON files for each AI tool to understand the current featureset and identify missing information.
4.  Research reviews and information: For each tool, I will use the `serper-google-search` tool to search for reviews, information, photos, sources, and places to buy. I will focus on identifying reliable sources (G2, Capterra, Gartner, Trustpilot, ProductHunt, industry-specific review sites) and Reddit posts. I will prioritize sources from the last 12 months and aim for at least 5 high-quality sources per tool.
5.  Scrape relevant URLs: I will use the `firecrawl_scrape` tool to extract information from the search results. If `firecrawl_scrape` fails or doesn't provide sufficient information, I will use the `fetch` tool as an alternative. I will log any failures. If multiple sources provide conflicting information, I will flag them for manual verification.
6.  Synthesize review content: I will analyze the scraped data and Reddit posts to create a detailed review section for each tool. The review will include:
    *   A summary of overall sentiment.
    *   Specific pros and cons mentioned by users. I will highlight trends in complaints/praise (e.g., "Most negative reviews focus on slow response time") and use specific user concerns instead of generic feedback (e.g., "Lacks API integration" is better than "Limited features"). I will cross-reference user feedback across different review platforms to identify consistent themes and discrepancies.
    *   Quotes from Reddit posts to represent average user opinions (e.g., "Some users think \_\_\_ while other users think \_\_\_").
    *   Ratings from different review sites (G2, Capterra, etc.). I will cross-reference user feedback across different review platforms to identify consistent themes and discrepancies, ensuring an objective analysis.
7.  Update the JSON files: I will use the `replace_in_file` tool to add the new, comprehensive review section to the corresponding JSON files in the `ai_tools_resource/data/` directory.
8.  Add photos: I will use the `website/img/` directory to store the photos and update the JSON files with the image paths. I will use descriptive filenames and alt text for the images, compress the images, and use modern formats (like WebP) to improve load times. I will also consider lazy loading images, ensure image dimensions match website need. I will ensure images have proper srcset attributes for responsive loading across devices, preventing unnecessary bandwidth usage on mobile
9.  Update website: I will update the website files (`website/tool-details.html`, `website/js/tool-details.js`, etc.) to display the new information. I will ensure that the website uses internal linking to connect relevant pages and is mobile-optimized. I will also optimize metadata (title tags, descriptions, and headings) with relevant keywords and add structured data (Schema.org markup) to improve search engine visibility for tool reviews.
10. Quality Assurance (QA): After updating the JSON and website files, I will perform a manual review to catch formatting errors, broken links, and data inconsistencies.
11. SEO Strategy: Implement analytics to track SEO improvements over time. I will use Google Search Console & Core Web Vitals to track and optimize for technical SEO and optimize for featured snippets (e.g., use clear Q&A formatting for "What is \[tool] used for?").
12. Consider edge cases: I will analyze the updated data and identify any edge cases or potential issues. I will also establish a periodic review schedule (e.g., quarterly updates) to keep data fresh and create a failure-handling protocolâ€”what happens if a tool is discontinued or data sources become unreliable?
